{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxcWWRms0adP"
      },
      "source": [
        "DeepAI Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "SBq_NTdJepdr",
        "outputId": "821b7f96-cb0e-448a-93e9-68fe1579f087"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"This dataset contains information on dairy cows from various farms within a specific region. The dataset consists of the cows' row IDs, location by Woreda and Kebele, live weight, pregnancy months, milk volume, fat and protein content, lactation stage, as well as feed data like hay, corn, wheat bran and linseed cake and costs. Additionally, information on milk income over feed costs per day (MIFC day) and ration prices are also included. This data can be used to examine the relationship between feed and milk production, as well as factors that may impact profitability.\""
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'ROW_ID': [1, 2],\n",
        "    'Rice': ['######', '######'],\n",
        "    'WOREDA': ['Woreda1', 'Woreda1'],\n",
        "    'KEBELE': ['Kebele1', 'Kebele1'],\n",
        "    'LIVE_WEIGHT': [325, 325],\n",
        "    'PREGNANCY_MTH': [1, 1],\n",
        "    'MILK_VOLUME': [9, 10],\n",
        "    'MILK_FAT': [4, 4],\n",
        "    'MILK_PROTEIN': [2.6, 2.6],\n",
        "    'LACTATION_STAGE': ['Early lactation', 'Early lactation'],\n",
        "    'HAY': [2.65, 3.38],\n",
        "    'CORN': [0, 0],\n",
        "    'WHEAT_BRAN': [4.5, 5],\n",
        "    'LINSEED_CAKE': [2.69, 2.25],\n",
        "    'FEED_COST': [283.25, 279.31],\n",
        "    'MIFC_DAY': [22.75, 60.69],\n",
        "    'RATION_PRICE': [28.79, 26.28]\n",
        "})\n",
        "\n",
        "def concat_column_names(columns):\n",
        "    concatenated_names = ', '.join(columns)\n",
        "    return concatenated_names\n",
        "\n",
        "all_cols = concat_column_names(data.columns.values)\n",
        "\n",
        "r = requests.post(\n",
        "    \"https://api.deepai.org/api/text-generator\",\n",
        "    data={\n",
        "        'text': f'generate a description for a dataset, with columns such as : {all_cols}',\n",
        "    },\n",
        "    headers={'api-key': 'quickstart-QUdJIGlzIGNvbWluZy4uLi4K'}\n",
        "\n",
        "\n",
        ")\n",
        "r.json()['output']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxSs-NhkQS9a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDYKz57g0jse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_3bhD1lQqUi"
      },
      "source": [
        "Pandasai Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjLtDAQPQuby"
      },
      "outputs": [],
      "source": [
        "!pip install pandasai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOs7u2EBRB05"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandasai import PandasAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRhmRSecRpXo",
        "outputId": "17931b9e-3069-43c8-95a3-f5b7554fcc5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Incorrect API key provided: YOUR_OPE*******_KEY. You can find your API key at https://platform.openai.com/account/api-keys.\n",
            "Unfortunately, I was not able to answer your question, because of the following error:\n",
            "\n",
            "Incorrect API key provided: YOUR_OPE*******_KEY. You can find your API key at https://platform.openai.com/account/api-keys.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Instantiate a LLM\n",
        "from pandasai.llm.openai import OpenAI\n",
        "\n",
        "# Sample DataFrame\n",
        "df_gdp = pd.DataFrame({\n",
        "    \"country\": [\"United States\", \"United Kingdom\", \"France\", \"Germany\", \"Italy\", \"Spain\", \"Canada\", \"Australia\", \"Japan\", \"China\"],\n",
        "    \"gdp\": [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],\n",
        "    \"happiness_index\": [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]\n",
        "})\n",
        "\n",
        "llm = OpenAI(api_token=\"YOUR_OPENAI_API_KEY\")\n",
        "pandas_ai = PandasAI(llm)\n",
        "\n",
        "# m = PandasAI(llm)\n",
        "# Output not correct since Germany is not the happiest country\n",
        "response = pandas_ai(df_gdp, prompt='What is the capital of the happiest country')\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7jsqgyqSPKe",
        "outputId": "b8574957-e29d-4d71-b0a6-e85434e11df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Canada\n"
          ]
        }
      ],
      "source": [
        "response2 = pandas_ai(df_gdp, prompt='What is the happiest country')\n",
        "print(response2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho-PhSsH8_ZL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKRBEy1l9Azx"
      },
      "source": [
        "Generating descriptions using LangChain (Pandas DataFrame Agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XT4REws_X74"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ArXfJos9HPB"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_pandas_dataframe_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents.agent_types import AgentType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob8WYHvLD5Zo"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0so9J-29fbh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('crop_cost.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1ncuKuzEk1o"
      },
      "outputs": [],
      "source": [
        "agent = create_pandas_dataframe_agent(\n",
        "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
        "    df,\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.OPENAI_FUNCTIONS\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxADMLeYD-8F",
        "outputId": "fc28d0db-393a-4c5b-9273-bc71f4244999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe dataset contains information about the cost of cultivation and production, as well as the yield of various crops in different states. \n",
            "\n",
            "Each row represents a specific crop in a specific state. The columns provide the following information:\n",
            "\n",
            "- Crop: The name of the crop.\n",
            "- State: The name of the state where the crop is cultivated.\n",
            "- Cost of Cultivation (`/Hectare) A2+FL`: The cost of cultivation per hectare, including all expenses except for the value of family labor (A2) and the value of leased-in labor (FL).\n",
            "- Cost of Cultivation (`/Hectare) C2`: The cost of cultivation per hectare, including all expenses, including the value of family labor (A2) and the value of leased-in labor (FL).\n",
            "- Cost of Production (`/Quintal) C2`: The cost of production per quintal (100 kilograms), including all expenses, including the value of family labor (A2) and the value of leased-in labor (FL).\n",
            "- Yield (Quintal/ Hectare): The yield of the crop per hectare in quintals.\n",
            "\n",
            "This dataset provides insights into the cost and yield of different crops in different states, which can be useful for analyzing agricultural practices and making informed decisions related to crop cultivation.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "text = agent.run(\"Provide a brief description of the dataset using all available information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG_zgoY6jBAb",
        "outputId": "b77f6310-fef8-43be-95f9-847791177856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The dataset contains information about the cost of cultivation and production, as well as the yield of various crops in different states. \n",
            "\n",
            "Each row represents a specific crop in a specific state. The columns provide the following information:\n",
            "\n",
            "- Crop: The name of the crop.\n",
            "- State: The name of the state where the crop is cultivated.\n",
            "- Cost of Cultivation (`/Hectare) A2+FL`: The cost of cultivation per hectare, including all expenses except for the value of family labor (A2) and the value of leased-in labor (FL).\n",
            "- Cost of Cultivation (`/Hectare) C2`: The cost of cultivation per hectare, including all expenses, including the value of family labor (A2) and the value of leased-in labor (FL).\n",
            "- Cost of Production (`/Quintal) C2`: The cost of production per quintal (100 kilograms), including all expenses, including the value of family labor (A2) and the value of leased-in labor (FL).\n",
            "- Yield (Quintal/ Hectare): The yield of the crop per hectare in quintals.\n",
            "\n",
            "This dataset provides insights into the cost and yield of different crops in different states, which can be useful for analyzing agricultural practices and making informed decisions related to crop cultivation.\n"
          ]
        }
      ],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkEmNEOllI0K",
        "outputId": "8ed87da4-c425-4b40-c76f-0767ef0a8fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.5.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install tiktoken\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "# from langchain.vectorstores import Chroma\n",
        "# from langchain.docstore.document import Document\n",
        "# from langchain.prompts import PromptTemplate\n",
        "# from langchain.indexes.vectorstore import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMICJQA2mCrb"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=100, length_function=len)\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "docsearch = FAISS.from_texts(texts, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T5oOCTE7mZkl",
        "outputId": "e1216307-da47-4d2e-9c0a-bbdbe5db7b2f"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquestion_answering\u001b[39;00m \u001b[39mimport\u001b[39;00m load_qa_chain\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> 4\u001b[0m chain \u001b[39m=\u001b[39m load_qa_chain(OpenAI(temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m query_2 \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mExtract the important keywords in this text\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m docs \u001b[39m=\u001b[39m docsearch\u001b[39m.\u001b[39msimilarity_search(query_2)\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/digital_green/lib/python3.10/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
            "File \u001b[0;32m~/.pyenv/versions/3.10.9/envs/digital_green/lib/python3.10/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
          ]
        }
      ],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "query_2 = \"Extract the important keywords in this text\"\n",
        "docs = docsearch.similarity_search(query_2)\n",
        "chain.run(input_documents=docs, question=query_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWzhCbfJlIlo"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_csv_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N29TCNJiEGHt",
        "outputId": "d5cea26a-0e82-4787-e331-3fbfd9796215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find columns with string datatypes and unique values less than 15\n",
            "Action: python_repl_ast\n",
            "Action Input: df.select_dtypes(include=['object']).nunique()\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mCrop                 55\n",
            "dtype: int64\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to filter out columns with unique values greater than 15\n",
            "Action: python_repl_ast\n",
            "Action Input: df.select_dtypes(include=['object']).nunique()[df.select_dtypes(include=['object']).nunique() < 15]\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSeries([], dtype: int64)\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m There are no columns with unique values less than 15\n",
            "Final Answer: There are no columns with unique values less than 15.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.agents.agent import Agent\n",
        "from langchain.agents import agent_types\n",
        "df_2 = pd.read_csv('crop_cost.csv')\n",
        "agent_2 = create_csv_agent(\n",
        "    OpenAI(temperature=0),\n",
        "    \"crop_yield.csv\",\n",
        "    verbose=True,\n",
        "    agent_types = AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")\n",
        "\n",
        "query = agent_2.run(\"Return a dictionary with the keys as column names of string datatypes with unique values less than 15 and value as a list of unique values in that column\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "T6ETqtgQolTC",
        "outputId": "5bb148d1-3c1c-4f07-871d-c32cad6dffac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"{'Crop': ['ARHAR', 'COTTON', 'GRAM', 'GROUNDNUT', 'MAIZE', 'MOONG', 'SAFFLOWER', 'SESAMUM', 'SORGUM', 'WHEAT'], 'State': ['Uttar Pradesh', 'Karnataka', 'Gujarat', 'Andhra Pradesh', 'Maharashtra', 'Rajasthan', 'Tamil Nadu', 'Madhya Pradesh', 'Haryana', 'Punjab', 'Bihar', 'West Bengal', 'Odisha']}\""
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9g946N4qJ2vZ"
      },
      "outputs": [],
      "source": [
        "agent_2.run(\"Majority of the states in this dataset is from which country\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6SpXG_YKLCa"
      },
      "outputs": [],
      "source": [
        "agent.run(\"Generate relevant keywords from this dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6riwrNULuLP"
      },
      "outputs": [],
      "source": [
        "agent_2.run(\"Generate relevant keywords from this dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHz7y5BKPgUg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5XgnXPfywYt"
      },
      "source": [
        "ReAct Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_KN237Pyv0j"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3I6el65yg2x"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "agent = create_pandas_dataframe_agent(\n",
        "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
        "    df,\n",
        "    verbose=True,\n",
        "    gent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")\n",
        "agent.run(\"Provide a brief description of the dataset using all available information\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP8i2Vzfygje"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksQjDWofJgpX"
      },
      "outputs": [],
      "source": [
        "def extract_data(json_content):\n",
        "    \"\"\"\n",
        "    Extracts the title, descripton, id and youtube id from the json object.\n",
        "\n",
        "    :param json_content: a JSON object with the video metadata\n",
        "    :return: a tuple with the title, descripton, id and youtube id\n",
        "    \"\"\"\n",
        "    return (json_content['base']['title'], json_content['base']['description'], json_content['id'], json_content['youtube_id'])\n",
        "\n",
        "\n",
        "def process_all_records(batch_size: int = 100):\n",
        "    \"\"\"\n",
        "    Generator function which loops through all videos via a REST interface.\n",
        "\n",
        "    :param batch_size: The size of the batch retrieved via the REST interface call.\n",
        "    \"\"\"\n",
        "    start = 0\n",
        "    while True:\n",
        "        response = requests.get(f\"https://admin.thelighthouse.world/videos?_limit={batch_size}&_start={start}\")\n",
        "        json_content = response.json()\n",
        "        for jc in json_content:\n",
        "            # Pass the content to the caller using the generator with yield\n",
        "            yield extract_data(jc)\n",
        "        if len(json_content) < batch_size:\n",
        "            break\n",
        "        start += batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIIjt8UXJgRh"
      },
      "outputs": [],
      "source": [
        "records = process_all_records()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OL3rP5mKxdo",
        "outputId": "f9fdce4e-3f4f-47f0-a474-6d459414162e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<generator object process_all_records at 0x7f0a91da42e0>\n"
          ]
        }
      ],
      "source": [
        "print(records)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjm1K-tEJm8q"
      },
      "outputs": [],
      "source": [
        "def extract_keywords_from_chat(chat, record_data):\n",
        "    \"\"\"\n",
        "    Sends a chat question to ChatGPT and returns its output.\n",
        "\n",
        "    :param chat: The object which communicates under the hood with ChatGPT.\n",
        "    :param record_data: The tuple with the title, description, id and youtube id\n",
        "    \"\"\"\n",
        "    dt_single = f\"{record_data[0]} {record_data[1]}\"\n",
        "    resp = chat([\n",
        "                SystemMessage(content=\n",
        "                        \"You extract the main keywords in the text and extract these into a comma separated list. Please prefix the keywords with 'Keywords:'\"),\n",
        "                HumanMessage(content=dt_single)\n",
        "            ])\n",
        "    answer = resp.content\n",
        "    return dt_single,answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "l11I7c2hJt10",
        "outputId": "d4913dcb-b829-4f22-90a4-3e1f90d7b768"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e5766c5977dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextract_keywords_from_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: extract_keywords_from_chat() missing 2 required positional arguments: 'chat' and 'record_data'"
          ]
        }
      ],
      "source": [
        "extract_keywords_from_chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btrsHGKAJzmF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHDdaAXGt5NF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBHo3VFpt8_r"
      },
      "source": [
        "Extracting Keywords based on Data Analytics insights -- Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFYaumW9t5Qc",
        "outputId": "405072df-9a1d-496a-fcfc-bac9c3ae0c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. The majority of passengers were male (64.8%).\n",
            "2. The majority of passengers were in the third class (55.1%).\n",
            "3. The average age of passengers was 29.7 years.\n",
            "4. The majority of passengers embarked from Southampton (72.4%).\n",
            "5. The average fare paid was 32.2.\n"
          ]
        }
      ],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "titanic_df = pd.read_csv('/content/titanic_sub.csv')\n",
        "\n",
        "agent_titanic = create_pandas_dataframe_agent(\n",
        "    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n",
        "    titanic_df,\n",
        "    verbose=True,\n",
        "    agent_type=AgentType.OPENAI_FUNCTIONS\n",
        ")\n",
        "\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "prompt = PromptTemplate(\n",
        "    template=\"List top five data analytics insights from {subject} dataset.\\n{format_instructions}\",\n",
        "    input_variables=[\"subject\"],\n",
        "    partial_variables={\"format_instructions\": format_instructions}\n",
        ")\n",
        "\n",
        "model = OpenAI(temperature=0)\n",
        "\n",
        "_input = prompt.format_prompt(subject=titanic_df)\n",
        "output = model(_input.to_string())\n",
        "\n",
        "result = ', '.join(output_parser.parse(output))\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVGZb3RauXKN",
        "outputId": "4151f4a0-9833-4992-bcf1-8990e8104ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mBased on the insights gained, the keywords that can be extracted are:\n",
            "\n",
            "1. Male\n",
            "2. Third class\n",
            "3. Age\n",
            "4. Southampton\n",
            "5. Fare\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# output = agent.run(\"Give a brief description of the the dataset incorporating insights gained in\" +  result)\n",
        "output = agent_titanic.run(\"Extract keywords based on the insights gained from \"+ result)\n",
        "# output_1 = agent.run(\"Extract keywords based on the insights gained from \"+ result + \"excluding the numeric values \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhrnspYQvFnA",
        "outputId": "a05406e4-8a69-4111-b4ef-b55ea61afef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the insights gained, the keywords that can be extracted are:\n",
            "\n",
            "1. Male\n",
            "2. Third class\n",
            "3. Age\n",
            "4. Southampton\n",
            "5. Fare\n"
          ]
        }
      ],
      "source": [
        "print(output)\n",
        "# print(output_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvtfaTeZwZyA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGO3GYgvRWyt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m6SXBmlUu7a"
      },
      "source": [
        "Extracting keywords from columns with < 4 unique values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5YhnHNYVDbC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts1rtMgkSojC",
        "outputId": "2d00989c-f47f-403b-9c57-0b0d7d077c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.223-py3-none-any.whl (1.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.0/1.2 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.9-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.9 langchain-0.0.223 langchainplus-sdk-0.0.20 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# create an agent on the data\n",
        "!pip install langchain\n",
        "from langchain.agents import create_pandas_dataframe_agent\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents.agent_types import AgentType\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcOjCPttVAjP",
        "outputId": "db31d749-49fc-45ae-9256-a5cc614af4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "import pandas as pd\n",
        "df = pd.read_excel('SIMLESA_Project_data_standerdise.xlsx')\n",
        "\n",
        "llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\")\n",
        "agent = create_pandas_dataframe_agent(OpenAI(temperature=0),\n",
        "                                      df,\n",
        "                                      verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlPvF_BaYnvk"
      },
      "outputs": [],
      "source": [
        "# Ignore columns with numeric values\n",
        "non_numeric_df = df.select_dtypes(exclude='number')\n",
        "# Drop Field Name column without keeping copy\n",
        "non_numeric_df.drop('Field Name', axis=1, inplace=True)\n",
        "# non_numeric_df.head()\n",
        "\n",
        "# Get unique values from coluniques\n",
        "# Check whether they contain numbers\n",
        "import re\n",
        "to_delete_cols = []\n",
        "for col in non_numeric_df.columns:\n",
        "    my_list = non_numeric_df[col].unique()\n",
        "    for i in my_list:\n",
        "        if bool(re.search(\"[0123456789]\", str(i))) == True:\n",
        "            to_delete_cols.append(col)\n",
        "            break\n",
        "\n",
        "# Create new sub dataset\n",
        "cleaned_cols_df = non_numeric_df\n",
        "\n",
        "# Drop all columns having numerical values as string\n",
        "for col in cleaned_cols_df.columns:\n",
        "    if col in to_delete_cols:\n",
        "        cleaned_cols_df.drop(col, axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcLSv5OkWNEn",
        "outputId": "1fd1caa8-3956-430d-fdc0-a2a05fee7ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to find the columns with unique values less than 4 and get the unique words from those columns\n",
            "Action: python_repl_ast\n",
            "Action Input: df.nunique()\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3msite                  34\n",
            "agroecoregion          2\n",
            "original_treatment    51\n",
            "treatment              2\n",
            "association            3\n",
            "main_crop              3\n",
            "tillage_implement     10\n",
            "association_type      16\n",
            "legume_cultivar        9\n",
            "cropping_system        4\n",
            "tillage_practice       2\n",
            "geometry               3\n",
            "Treatment             55\n",
            "textural_class        41\n",
            "Textureclass          13\n",
            "Refined_texture        3\n",
            "drainage_class         4\n",
            "WRB class              9\n",
            "mulch_1                6\n",
            "mulc_2                 7\n",
            "Treatment.1           59\n",
            "dtype: int64\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find the columns with unique values less than 4\n",
            "Action: python_repl_ast\n",
            "Action Input: df.nunique()[df.nunique() < 4]\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3magroecoregion       2\n",
            "treatment           2\n",
            "association         3\n",
            "main_crop           3\n",
            "tillage_practice    2\n",
            "geometry            3\n",
            "Refined_texture     3\n",
            "dtype: int64\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to get the unique words from those columns\n",
            "Action: python_repl_ast\n",
            "Action Input: df[df.nunique()[df.nunique() < 4].index].apply(lambda x: x.unique().tolist()).to_dict()\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m{'agroecoregion': ['Highlands', 'Lowlands'], 'treatment': ['CA', 'Conv'], 'association': ['Sole', 'Rotation', 'Intercrop'], 'main_crop': ['Soya_bean', 'Maize', 'Harricot_bean'], 'tillage_practice': ['CA', 'Conv'], 'geometry': [nan, 'FLAT', 'RIDGE', 'Flat'], 'Refined_texture': ['Loam', nan, 'Clay', 'Sand']}\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: {'agroecoregion': ['Highlands', 'Lowlands'], 'treatment': ['CA', 'Conv'], 'association': ['Sole', 'Rotation', 'Intercrop'], 'main_crop': ['Soya_bean', 'Maize', 'Harricot_bean'], 'tillage_practice': ['CA', 'Conv'], 'geometry': [nan, 'FLAT', 'RIDGE', 'Flat'], 'Refined_texture': ['Loam', nan, 'Clay', 'Sand']}\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'agroecoregion': ['Highlands', 'Lowlands'], 'treatment': ['CA', 'Conv'], 'association': ['Sole', 'Rotation', 'Intercrop'], 'main_crop': ['Soya_bean', 'Maize', 'Harricot_bean'], 'tillage_practice': ['CA', 'Conv'], 'geometry': [nan, 'FLAT', 'RIDGE', 'Flat'], 'Refined_texture': ['Loam', nan, 'Clay', 'Sand']}\n"
          ]
        }
      ],
      "source": [
        "agent = create_pandas_dataframe_agent(OpenAI(temperature=0),\n",
        "                                      cleaned_cols_df,\n",
        "                                      verbose = True,\n",
        "                                      )\n",
        "result = agent.run(\"ignore columns with numerical values, get unique words from columns data where unique values are less than 4 and create a dictionary with the key as column names and their value as a list of unique values\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFNjtr0eZuil"
      },
      "source": [
        "Formating the output as list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrWchvheXnBV",
        "outputId": "dc05040b-1cb2-4283-df14-569ae05d9c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['agroecoregion', 'Highlands', 'Lowlands', 'treatment', 'CA', 'Conv', 'association', 'Sole', 'Rotation', 'Intercrop', 'main_crop', 'Soya_bean', 'Maize', 'Harricot_bean', 'tillage_practice', 'CA', 'Conv', 'geometry', 'nan', 'FLAT', 'RIDGE', 'Flat', 'Refined_texture', 'Loam', 'nan', 'Clay', 'Sand']\n"
          ]
        }
      ],
      "source": [
        "list_words = result.split()\n",
        "# clean the values from\n",
        "import re\n",
        "new_list = []\n",
        "for i in list_words:\n",
        "    a = re.sub('[^0-9a-zA-Z_]', \"\", i)\n",
        "    new_list.append(a)\n",
        "\n",
        "print(new_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN3M7-P3dssW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
